================================================================================
COMPREHENSIVE TD3 HOCKEY EXPERIMENT ANALYSIS
================================================================================

## EXPERIMENT OVERVIEW

All runs use:
- Same seed (55), opponent (weak), network size ([400,300] actor, [400,300,128] critic)
- Same batch size (1024), buffer size (1M)
- PBRS reward shaping enabled
- PER (Prioritized Experience Replay) enabled

Variables tested:
1. Learning Rate: 0.0003 vs 0.0005 (1.67x increase)
2. Tie Penalty: 0 (disabled) vs -1.5 (standard) vs -4.0 (2.67x increase)

## RESULTS SUMMARY

| Run Name            | LR     | Tie Pen | Episodes | Hrs  | Final Win% | Max Win% | Eval Weak | Eval Strong |
|---------------------|--------|---------|----------|------|------------|----------|-----------|-------------|
| baseline-sizes-rw   | 0.0003 | -1.5    | 25,900   | 9.8  | 17.92%     | 17.92%   | 33.82%    | 30.13%      |
| no-tie-pen          | 0.0003 | 0.0     | 23,650   | 8.4  | 19.81%     | 19.81%   | 37.06%    | 33.84%      |
| high-tie-pen        | 0.0003 | -4.0    | 26,550   | 8.4  | 19.47%     | 19.47%   | 32.33%    | 33.70%      |
| aggressive          | 0.0005 | -1.5    | 14,000   | 8.5  | 16.00%     | 16.00%   | 27.53%    | 29.88%      |
| agg-high-tie-pen    | 0.0005 | -4.0    | 13,150   | 8.5  | 17.07%     | 17.07%   | 36.17%    | 35.61%      |

## KEY FINDINGS

### 1. NO IMPROVEMENT FROM INTERVENTIONS
- **Baseline** (standard lr=0.0003, tie=-1.5): 17.92% win rate
- **Best performer**: no-tie-pen at 19.81% (only +1.89% improvement)
- All interventions hover around 16-20% win rate
- NO CLEAR WINNER emerged from any experiment

### 2. TIE PENALTY EFFECTS
**Removing tie penalty (0 vs -1.5):**
- Win rate: 19.81% vs 17.92% (+1.89%)
- Possession: 10.60% vs 4.72% (+124% increase!)
- Shoot action: -0.48 vs -0.14 (more keeping)
- Distance to puck: 3.09 vs 3.19 (closer)

**Increasing tie penalty (-4.0 vs -1.5):**
- Win rate: 19.47% vs 17.92% (+1.55%)
- Similar possession improvements
- NO significant advantage

**VERDICT**: Removing tie penalty slightly helped, but effect is marginal.

### 3. HIGHER LEARNING RATE (0.0005 vs 0.0003)
**Effects observed:**
- SLOWER episode throughput: 14k vs 26k episodes in same time
- Lower win rates: 16.00% vs 17.92% (baseline)
- More unstable Q-values: Q_avg range wider
- Actor loss instability: -0.35 to +1.50 vs 0 to +1.15

**VERDICT**: Higher LR (0.0005) HURT performance - caused instability and slower convergence.

### 4. COMBINED (lr=0.0005 + tie=-4.0)
- Win rate: 17.07% (middle of pack)
- Best eval vs weak: 36.17%
- Possession: 8.87% (high)
- Q-values: Most unstable (range: -1.60 to +1.52)

**VERDICT**: Combining aggressive changes didn't help - still ~17% win rate.

### 5. BEHAVIORAL PATTERNS

**Possession Strategy (shoot_action_when_possess):**
- All runs learned to KEEP puck (negative values)
- no-tie-pen: -0.48 (most keeping)
- agg-high-tie-pen: -0.42 (high keeping)
- baseline: -0.14 (moderate keeping)

**Distance to Puck:**
- Best: high-tie-pen at 3.01 (closest)
- Worst: agg-high-tie-pen at 3.24 (furthest)
- All within 0.23 units - minimal difference

### 6. Q-VALUE STABILITY

**Most stable:**
- baseline: Q_avg = -0.87 (consistent negative)
- high-tie-pen: Q_avg = -0.67

**Most unstable:**
- agg-high-tie-pen: -1.60 to +1.52 (wildly swinging)
- no-tie-pen: -1.72 to +1.45 (wide range)

**Higher LR caused Q-value instability across all metrics.**

### 7. EVAL PERFORMANCE ANOMALY
**All runs show: win_rate vs STRONG ‚âà win_rate vs WEAK**

This persistent anomaly suggests:
- Strong opponent's aggression creates chaos
- Agent exploits strong opponent's predictable attacks
- Weak opponent (kp=0.5) might be harder to score against

## CONCLUSIONS

### ‚ùå What DIDN'T Work:
1. **Higher Learning Rate (0.0005)**: Caused instability, slower convergence, worse performance
2. **Higher Tie Penalty (-4.0)**: No significant benefit over standard -1.5
3. **Combined interventions**: No synergistic effect

### ‚úÖ What Marginally Helped:
1. **Removing tie penalty**: +1.89% win rate, +124% possession
   - Side effect: Agent holds puck more, gets closer
   - But this might be coincidental, not causal

### üîç Core Problem: ALL EXPERIMENTS PLATEAU AT 16-20%
- No experiment broke through 20% win rate
- Eval performance (27-37% vs weak) much better than training (16-20%)
- This suggests: **exploration/generalization issue, not hyperparameter issue**

### üéØ Recommendations:
1. **Don't pursue higher LR** - clearly harmful
2. **Tie penalty doesn't matter much** - save compute, use standard -1.5 or remove
3. **Real bottleneck is elsewhere:**
   - Network architecture? (already at [400,300])
   - Reward shaping strategy?
   - Training curriculum (need self-play)?
   - Exploration strategy (epsilon schedule)?

### üìä Statistical Significance:
With 1.89% difference between best and worst, and all runs using same seed:
- Differences are likely NOISE, not signal
- **None of these hyperparameters made a meaningful difference**
- Need to try fundamentally different approaches (self-play, curriculum, etc.)

================================================================================
## DETAILED METRIC COMPARISONS
================================================================================

### Win Rate Progression (Training)
baseline-sizes-rw:   0% ‚Üí 3.5% ‚Üí 10% ‚Üí 17.92% (25.9k eps)
no-tie-pen:          0% ‚Üí 3.5% ‚Üí 12% ‚Üí 19.81% (23.7k eps) ‚úì BEST
high-tie-pen:        0% ‚Üí 3.8% ‚Üí 12% ‚Üí 19.47% (26.6k eps)
aggressive:          0% ‚Üí 3.7% ‚Üí 10% ‚Üí 16.00% (14.0k eps) ‚úó WORST  
agg-high-tie-pen:    0% ‚Üí 3.8% ‚Üí 11% ‚Üí 17.07% (13.2k eps)

### Eval Win Rate vs Weak (Best Performance)
no-tie-pen:          37.06% ‚úì BEST
agg-high-tie-pen:    36.17%
baseline-sizes-rw:   33.82%
high-tie-pen:        32.33%
aggressive:          27.53% ‚úó WORST

### Possession Metrics
no-tie-pen:          10.60% possession ‚úì BEST
agg-high-tie-pen:    8.87%
high-tie-pen:        7.45%
aggressive:          8.10%
baseline-sizes-rw:   4.72% ‚úó WORST

### Q-Value Stability (smaller range = more stable)
high-tie-pen:        Q range 2.12 ‚úì MOST STABLE
baseline-sizes-rw:   Q range 2.76
aggressive:          Q range 3.17
no-tie-pen:          Q range 3.17
agg-high-tie-pen:    Q range 3.11 ‚úó LEAST STABLE

### Episodes/Hour Throughput
baseline-sizes-rw:   2,643 eps/hr ‚úì FASTEST
high-tie-pen:        3,160 eps/hr
no-tie-pen:          2,816 eps/hr
agg-high-tie-pen:    1,547 eps/hr ‚úó SLOWEST (-42%)
aggressive:          1,647 eps/hr ‚úó SLOWEST (-38%)

**CRITICAL FINDING: Higher LR cuts throughput in HALF**

================================================================================
## PATTERN ANALYSIS
================================================================================

### What Correlates with Better Performance?
1. ‚úó Learning rate 0.0005: NEGATIVE correlation (-1.92% vs baseline)
2. ‚úì No tie penalty: POSITIVE correlation (+1.89% vs baseline)
3. ‚úó High tie penalty (-4.0): Neutral/slight positive (+1.55%)
4. ‚úì More episodes: Weak positive (more training = slightly better)

### What Doesn't Matter?
1. Tie penalty magnitude: -1.5 vs -4.0 = negligible difference
2. Action magnitude: All runs converge to ~1.1-1.2
3. Distance to puck: All within 0.3 units (10% variance)

### Critical Insight: Training vs Eval Gap
All runs show 2x better performance in EVAL than TRAINING:
- Training win rate: 16-20%
- Eval win rate: 27-37%

This gap suggests:
1. **Evaluation bias**: Fixed eval opponent easier than dynamic training opponent
2. **Exploration hurt**: High epsilon during training hurts win rate
3. **Position alternation**: Training alternates, eval doesn't (tournament mode)

================================================================================
## FINAL VERDICT
================================================================================

### üéØ RECOMMENDED SETTINGS (minimal changes from baseline):
- Learning rate: **0.0003** (confirmed optimal)
- Tie penalty: **0** or **-1.5** (doesn't matter, pick one)
- Batch size: **1024** (already optimal)
- Network size: **[400,300]** actor, **[400,300,128]** critic (already optimal)

### ‚ö†Ô∏è AVOID:
- Learning rate 0.0005 or higher (causes instability + 40% slower)
- Extreme tie penalties (-4.0+) (no benefit, just noise)

### üöÄ NEXT STEPS (to break 20% plateau):
1. **Enable self-play** (curriculum learning against past selves)
2. **Increase training episodes** to 50k+ (more data)
3. **Adjust epsilon schedule** (faster decay, or anneal to 0.05)
4. **Try different opponents** (train vs strong, not just weak)
5. **Investigate eval/train gap** (why 2x better in eval?)

### üìâ HARSH TRUTH:
**None of your hyperparameter experiments significantly improved performance.**

The 1.89% gain from removing tie penalty is likely within noise margin.
Real breakthrough requires:
- More training data (50k+ episodes)
- Better curriculum (self-play)
- Or fundamentally different approach

================================================================================
