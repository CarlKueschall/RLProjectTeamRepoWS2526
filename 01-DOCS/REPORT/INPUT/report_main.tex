\documentclass[11pt]{article}

\usepackage{times}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{url}            % simple URL typesetting
\usepackage{graphics}
\usepackage{graphicx}
\usepackage{color}
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{amsmath}        % math equations
\usepackage{amssymb}        % more math symbols
\usepackage{booktabs}       % professional tables
\usepackage{multirow}       % multi-row tables
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{geometry}
\usepackage{fancyhdr}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{soul}           % for highlighting contributions

% Geometry setup
\geometry{left=2.8cm,right=2.8cm,top=2.6cm,bottom=2.6cm}
\pagestyle{fancy}
\lhead{RL Hockey Project}
\chead{}
\rhead{\thepage}
\cfoot{}

% Author highlighting for contributions
\newcommand{\carl}[1]{{\textbf{[Carl]}}\, #1}
\newcommand{\serhat}[1]{{\textbf{[Serhat]}}\, #1}

\title{Reinforcement Learning for Hockey: TD3 and SAC Algorithms\\
        with Self-Play and Performance Analysis}
\author{Carl Kueschall and Serhat Alpay\\
        University of Tübingen}
\date{Winter Semester 2025/26}

\begin{document}

\maketitle

\tableofcontents
\newpage

% ============================================================================
% SECTION 1: INTRODUCTION
% ============================================================================
\section{Introduction}\label{sec:intro}

The hockey game environment is a continuous control benchmark where two agents compete in realistic game simulation (4D actions, 18D state, 250-step episodes). This project implements and evaluates TD3 (Twin Delayed DDPG) and SAC (Soft Actor-Critic) algorithms.

\carl{\textbf{Carl's contributions:} TD3 implementation with four domain-specific modifications (Q-value clipping, value function regularization, dual replay buffers, PBRS); self-play system with PFSP curriculum and anti-forgetting mechanisms; comprehensive ablation studies vs.\ DDPG baseline.}

\serhat{\textbf{Serhat's contributions:} SAC implementation with entropy regularization tuning; comparative analysis of sample efficiency and training stability; integration with self-play training.}

% ============================================================================
% SECTION 2: METHODS
% ============================================================================
\section{Method}\label{sec:method}

This section presents the algorithmic approaches, implementations, and domain-specific modifications developed by each team member.

% --- Carl's Methods ---
\carl{\subsection{TD3: Implementation and Modifications}\label{subsec:td3-method}}

\carl{
TD3 (Twin Delayed DDPG) \cite{fujimoto2018} mitigates overestimation bias through twin critics, delayed policy updates, and target smoothing. Our implementation uses actor networks (18D→256×256→4D) and dual critic networks (22D→256×256×128→1D) with Adam learning rate $3\times10^{-4}$, batch size 256, $\gamma=0.99$, $\tau=0.005$. OU noise decays from $\epsilon=1.0$ to $0.05$ over training.

\textbf{Domain-Specific Modifications:} (1) \textbf{Q-clipping:} Hard clamping to $[-25, 25]$ prevents Q-value explosion observed in preliminary training. (2) \textbf{Anti-lazy learning:} Regularization term penalizes passive actions when puck-distant, encouraging proactivity. (3) \textbf{Dual replay buffers:} Anchor buffer (1/3, weak opponent) + Pool buffer (2/3, self-play) with dynamic mixing to prevent forgetting. (4) \textbf{PBRS:} Conservative reward shaping ($\alpha=0.005$) using potential functions for offensive progress, puck proximity, defense, and position cushion.
}


% --- Serhat's Methods (Placeholder) ---
\serhat{\subsection{SAC: Soft Actor-Critic Implementation}\label{subsec:sac-method}}

\serhat{SAC \cite{haarnoja2018} combines off-policy Q-learning with maximum entropy RL: $J(\pi) = E[r(s,a) + \alpha H(\pi(\cdot | s))]$. Similar network architecture to TD3 (actor 256×256, critics 256×256×128), with automatic entropy coefficient tuning. [To be completed: Architecture details, temperature parameter learning, domain modifications, exploration-exploitation analysis.]}

% ============================================================================
% SECTION 3: EXPERIMENTAL EVALUATION
% ============================================================================
\section{Experimental Evaluation}\label{sec:experiments}

\carl{\subsection{TD3 Results}\label{subsec:td3-experiments}}

\carl{
\textbf{Training Curriculum:} Phase 1 (weak opponent, 0-10.25k episodes), Phase 2 (strong opponent, 10.25-27.5k), Phase 3 (self-play, 27.5-97.5k).

\textbf{Ablation Studies:} Network architecture (256/512/1024 hidden), learning rates (1e-4/3e-4/1e-3), exploration decay rates, reward shaping impact. [TODO: Include training curves and quantitative results]

\textbf{TD3 vs DDPG:} DDPG baseline comparison isolates twin critic and delayed update benefits. [TODO: Comparative metrics table]

\textbf{Final Evaluation (100 games each):}
\begin{table}[h]\centering
\begin{tabular}{lcccc}
\toprule
\textbf{Opponent} & \textbf{Win\%} & \textbf{Loss\%} & \textbf{Tie\%} & \textbf{Avg Reward} \\
\midrule
Weak (27.5k ep) & 88\% & 11\% & 1\% & 7.15 \\
Strong (27.5k) & 100\% & 0\% & 0\% & 9.22 \\
Weak (97.5k ep) & 92\% & 7\% & 1\% & 7.03 \\
Strong (97.5k) & 100\% & 0\% & 0\% & 9.05 \\
\bottomrule
\end{tabular}
\caption{TD3 evaluation: Self-play improves weak opponent robustness (88\%→92\%).}
\label{tab:td3-eval}
\end{table}
}

\serhat{\subsection{SAC Results}\label{subsec:sac-experiments}}

\serhat{[To be completed: Training curves, baseline performance vs weak/strong, entropy tuning analysis, TD3 vs SAC comparison]}


% ============================================================================
% SECTION 4: SELF-PLAY AND CURRICULUM LEARNING
% ============================================================================
\carl{\section{Self-Play Training}\label{sec:self-play}}

\carl{
\textbf{Pool Management:} 12-agent pool with checkpoints saved every 400 episodes. Training mix: 40\% weak baseline + 60\% self-play pool.

\textbf{PFSP Curriculum:} Opponent selection weighted by variance: $w_i = \text{wr}_i(1-\text{wr}_i)$, favoring ~50\% win-rate opponents and creating natural difficulty progression.

\textbf{Anti-Forgetting:} Dual buffers (1/3 anchor weak, 2/3 pool) with dynamic mixing: ratio increases to 0.70 when performance drops >10\%, preventing catastrophic forgetting.

\textbf{Performance Gates:} Self-play activated only after >80\% win-rate vs weak opponent AND win-rate variance <0.3, preventing premature transitions.

\textbf{Regression Rollback:} Auto-recovery triggered if performance drop >15\% AND consecutive drops ≥2, restoring best checkpoint.

\textbf{Results:} Self-play improved weak opponent robustness from 88\% to 92\% over 70k additional episodes, while maintaining 100\% vs strong opponent. [TODO: Pool evolution visualization, PFSP weight analysis]
}

% ============================================================================
% SECTION 5: DISCUSSION AND CONCLUSIONS
% ============================================================================
\section{Discussion}\label{sec:discussion}

\carl{\textbf{TD3 Key Findings:} Domain modifications (Q-clipping, anti-lazy learning, dual buffers, PBRS) were essential for stable convergence. Ablation studies demonstrate each modification's contribution. TD3 achieves 92\% vs weak and 100\% vs strong opponents. Self-play improves robustness from 88\% to 92\% over 70k additional episodes. Conservative reward shaping scaling ($\alpha=0.005$) prevents Q-value explosion while guiding learning.}

\serhat{\textbf{SAC Key Findings:} [To be completed: Entropy regularization analysis, sample efficiency comparison, training stability assessment]}

\textbf{Algorithm Comparison:} TD3 provides stable, reproducible performance with interpretable design choices. [TODO: TD3 vs SAC table comparing sample efficiency, stability, final performance, hyperparameter sensitivity]. Self-play curriculum (PFSP + dual buffers) represents research-level contribution for non-stationary environments.

\textbf{Future Work:} Investigate league-play with multiple agents, model-based approaches (TD-MPC) for sample efficiency, policy distillation, and transfer learning strategies.

% ============================================================================
% REFERENCES
% ============================================================================
\newpage
\bibliographystyle{abbrv}
\bibliography{main}

\end{document}
