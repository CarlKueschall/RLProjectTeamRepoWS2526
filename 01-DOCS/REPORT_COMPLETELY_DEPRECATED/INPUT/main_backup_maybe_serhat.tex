\documentclass[11pt]{article}

\usepackage{times}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{url}            % simple URL typesetting
\usepackage{graphics}
\usepackage{graphicx}
\usepackage{color}
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{amsmath}        % math equations
\usepackage{amssymb}        % more math symbols
\usepackage{booktabs}       % professional tables
\usepackage{multirow}       % multi-row tables
\usepackage{subcaption}     % For subfigure environment
\usepackage{caption}        % For subcaption styling
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{geometry}
\usepackage{fancyhdr}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{soul}           % for highlighting contributions

% Geometry setup
\geometry{left=2.8cm,right=2.8cm,top=2.6cm,bottom=2.6cm}
\pagestyle{fancy}
\lhead{RL Hockey Project}
\chead{}
\rhead{\thepage}
\cfoot{}

% Author highlighting for contributions
\newcommand{\carl}[1]{{\textbf{[Carl Kueschall]}}\, #1}
\newcommand{\serhat}[1]{{\textbf{[Serhat Alpay]}}\, #1}

\title{Reinforcement Learning for Hockey: TD3 and SAC Algorithms\\
        with Self-Play and Performance Analysis}
\author{Carl Kueschall and Serhat Alpay\\
        University of Tübingen}
\date{Winter Semester 2025/26}

\begin{document}

\maketitle

\tableofcontents
\newpage

% ============================================================================
% SECTION 1: INTRODUCTION
% ============================================================================
\section{Introduction}\label{sec:intro}

\carl{In this project we work with an environment simulating an air-hockey game \cite{martius2024}. It presents a challenging test of continuous control in an adverserial situation. More specifically, the environment features a 4D continuous action space (per player), and an 18D state space with positions, velocities, puck state etc. An episode has a maximum length of 250 steps with a default sparse reward of +10 for a scored goal + options for reward shaping, built into the environment engine. The environment uses the popular Gymnasium API \cite{towers2024}. In summary, the environment requires the agents to learn complex behaviors with regards to positioning, puck control, balancing aggressive against defensive moves and more.} 
% ========================

\serhat{The project implements and evaluates two deep reinforcement learning algorithms that can be considered state-of-the-art. Carl Kueschall implemented TD3 (Twin Delayed DDPG) \cite{fujimoto2018}, which improves upon the DDPG algorithm, while Serhat Alpay implemented the SAC (Soft Actor-Critic) \cite{haarnoja2018}. Both of these are actor-critic methods designed for specifically continuous control tasks, but they employ different strategies for exploration, value estimation and also policy optimization.}
% ========================

\carl{\textbf{Carl Kueschall's contributions:} Implementation of the TD3 algorithm, with four major domain-specific modifications, which address Q-value instability, hindering lazy learning behavior, catastrophic forgetting (especially in self-play), and the big challenge in this kind of environment—sparse rewards. Developed comprehensive self-play system with Prioritized Fictitious Self-Play (PFSP), curriculum and anti-forgetting mechanisms like the dual replay buffers, performance gating and also regression rollback to prevent stalling. Conducted ablations examining network architecture scaling, the impact of reward-shaping efforts, and the effectiveness of the self-play implementation. The final benchmark scores were a winrate of 92\% against the weak opponent and 100\% against the strong opponent.}
% ========================

% ===== TODO: REWRITE THIS PARAGRAPH IN YOUR OWN WORDS =====
% Current text (needs rewriting):
\serhat{\textbf{Serhat Alpay's contributions:} SAC implementation with automatic entropy regularization tuning for optimal exploration-exploitation balance. Comparative analysis of sample efficiency and training stability between SAC and TD3. Integration of SAC with self-play training framework. Evaluation of entropy coefficient adaptation and its impact on policy diversity and performance.}
% ===== END SECTION TO REWRITE =====

% ============================================================================
% SECTION 2: METHODS
% ============================================================================
\section{Method}\label{sec:method}

The next section outlines the algorithmic approaches, different implementations and all the modifications developed by each team member.
% ========================

% --- Carl Kueschall's Methods ---
\carl{\subsection{TD3: Implementation and Modifications}\label{subsec:td3-method}}

\carl{
TD3 (Twin Delayed DDPG) \cite{fujimoto2018} represents the obvious step-up from DDPG, because it addresses the core flaws of its parent algorithm. It addresses three key issues through these innovations: (1) The twin critic networks which now take the minimum Q-value to reduce the overestimation bias present in DDPG. (2) The delayed policy updates which further prevent the policy from chasing the value function as it moves and optimizes. (3) Smoothing of the target policy using clipped noise. It is meant to add robustness without reintroducing the problematic overestimation.

\textbf{Objective Functions:} TD3 optimizes twin Q-functions $Q_1$ and $Q_2$ using the Bellman target with target policy smoothing:
\begin{equation}
y = r + \gamma \min_{i=1,2} Q_i'(s', \pi'(s' + \epsilon)) \cdot (1 - d)
\end{equation}
where $\epsilon \sim \text{clip}(\mathcal{N}(0, \sigma), -c, c)$ is clipped Gaussian noise, $d$ is the done flag, and $Q_i'$ and $\pi'$ are target networks. Each critic network is updated independently using Smooth L1 loss (Huber loss):
\begin{equation}
\mathcal{L}_{Q_i} = \mathbb{E}_{(s,a,r,s',d) \sim \mathcal{D}} \left[ \text{SmoothL1}(Q_i(s, a), y) \right] + \mathcal{L}_{\text{vf-reg},i}
\end{equation}
for $i \in \{1, 2\}$, where $\mathcal{L}_{\text{vf-reg},i}$ is the anti-lazy learning regularization term applied to critic $i$. The actor loss maximizes the minimum Q-value estimate:
\begin{equation}
\mathcal{L}_{\text{actor}} = -\mathbb{E}_{s \sim \mathcal{D}} \left[ \min_{i=1,2} Q_i(s, \pi(s)) \right]
\end{equation}
where $\mathcal{D}$ is the replay buffer. The actor is updated every $d$ critic updates (delayed policy updates, $d=2$ in our implementation).
% ========================


Our implementation used actor and critic networks with the following architecture. Actor: 18D→1024×1024→4D. Critic: 22D→1024×1024×200→1D. The training hyperparameters were optimized through iterative experimentation and ablation studies but were not put through extensive grid-search optimization, meaning that some performance is very likely left to be grasped through even better configuration. Best found hyperparameters: Adam optimizer with learning rate $3\times10^{-4}$ for both actor and critic, batch size 512, discount factor $\gamma=0.99$, soft update coefficient $\tau=0.005$, policy update frequency of 2 (update policy every 2 critic updates), target noise $\sigma=0.2$, and noise clip $c=0.5$. Ornstein-Uhlenbeck exploration noise \cite{lillicrap2015} decays exponentially from $\epsilon=1.0$ to $0.05$ over the course of training (decay factor $0.99995$ per episode). Replay buffer size is 500k. All networks are implemented in PyTorch \cite{paszke2019}, and experiments are tracked using Weights \& Biases \cite{biewald2020}.
% ========================

\textbf{Domain-Specific Modifications:} 

\textbf{Q-value clipping:} We hard clamp Q-values to $[-25, 25]$. This prevents Q-value explosion which we observed in early training experiments where the values exceeded 1000 and beyond. This new scale is additionally justified by the reward scale in the environment (even with our reward shaping efforts). The maximum episode reward remains approximately $\pm 10$.
% ========================

\textbf{Anti-lazy learning regularization:} A specific regularization term $\mathcal{L}_{\text{vf-reg}} = \lambda_{\text{vf}} \cdot \max(0, Q(s, a_{\text{passive}}) - Q(s, a_{\text{active}}))$ was introduced to combat annoying lazy-learning behavior, where $a_{\text{passive}} = [0,0,0,0]$ is the passive action and $a_{\text{active}} = \pi(s)$ is the learned action. It is meant to penalize the agent in a simple yet effective way, by punishing when it learns that a passive action receives higher Q-value than the learned action. It is applied only then when the distance to the puck exceeds a threshold (3.0 units). This addresses an early observation, that without extra incentive, the agent learns to be passive, as the sparse reward only rarely trickles through. Additional experiments/ablations could reveal whether this enhancement is necessary considering our additional reward shaping. Regularization strength $\lambda_{\text{vf}} = 0.1$.  
% ========================


\textbf{Dual replay buffers with dynamic mixing:} We use two separate buffers to maintain experiences, specifically for self-play to work well. One anchor-buffer as it is often called in RL, because it holds the core target experiences (in our case transitions playing against the weak or strong bot) and a pool buffer which contains the recent self-play interactions. In training we can then sample 1/3 from the first anchor buffer and the other 2/3 from the pool buffer. Dynamic mixing adaptively increases anchor ratio to 0.70 when performance drop from peak exceeds 10\%, preventing catastrophic forgetting which was observed in early self-play experiments where performance degraded from 92\% to 75\%.
% ========================

\textbf{Potential-Based Reward Shaping (PBRS):} We also use something called conservative reward shaping \cite{ng1999} with the scaling factor $\alpha=0.005$ using four potential function components: $\Phi_{\text{off}}(s) = 1.5 \times (1.0 - \min(\text{dist\_to\_opponent\_goal} / 10.0, 1.0))$ for offensive progress, $\Phi_{\text{prox}}(s) = -1.5 \times \tanh(\text{dist\_to\_puck} / 1.5)$ aiming for proximity to the puck, $\Phi_{\text{defense}}(s) = 1.0 \times \exp(-(\text{dist\_to\_lane})^2 / 0.5)$ for defensive positioning, and $\Phi_{\text{cushion}}(s) = -2.0 \times \tanh(\text{ReLU}(p_x + 2.0))$ to punish risky positions. On top of that we introduce strategic bonuses: reward puck touches (+0.06), goal-directed movement (+0.12), clear shots (+0.15), and penalize blocked shots (-0.20). The conservative $\alpha=0.005$ prevents reward hacking while providing dense learning signal. Potential-based shaping preserves the set of optimal policies \cite{ng1999}. This approach was chosen precisely to deal better with the issue of reward hacking and unintentional behavior induced through reward shaping. See Appendix~\ref{app:reward-shaping} for complete component details.
% ========================

}

% --- Serhat Alpay's Methods ---
\serhat{\subsection{SAC: Soft Actor-Critic Implementation}\label{subsec:sac-method}}

\serhat{
% ===== TODO: REWRITE THIS PARAGRAPH IN YOUR OWN WORDS =====
% Current text (needs rewriting):
SAC (Soft Actor-Critic) \cite{haarnoja2018} combines off-policy Q-learning with maximum entropy reinforcement learning, optimizing the objective $J(\pi) = E_{\tau \sim \pi}[r(s,a) + \alpha H(\pi(\cdot | s))]$ where $H(\pi(\cdot | s))$ is the entropy of the policy distribution and $\alpha$ is the temperature parameter controlling the trade-off between reward maximization and entropy maximization. The entropy term encourages exploration by favoring policies with higher entropy, naturally balancing exploration and exploitation.

\textbf{Objective Functions:} SAC maintains twin Q-functions $Q_1$ and $Q_2$ updated using the soft Bellman target:
\begin{equation}
y = r + \gamma \left( \min_{i=1,2} Q_i'(s', a') - \alpha \log \pi(a' | s') \right) \cdot (1 - d)
\end{equation}
where $a' \sim \pi(\cdot | s')$ is sampled from the current policy and $Q_i'$ are target Q-networks. The Q-function loss is:
\begin{equation}
\mathcal{L}_{Q_i} = \mathbb{E}_{(s,a,r,s',d) \sim \mathcal{D}} \left[ \frac{1}{2} (Q_i(s, a) - y)^2 \right]
\end{equation}
for $i \in \{1, 2\}$. The actor loss maximizes entropy-regularized Q-value:
\begin{equation}
\mathcal{L}_{\text{actor}} = \mathbb{E}_{s \sim \mathcal{D}, a \sim \pi(\cdot | s)} \left[ \alpha \log \pi(a | s) - \min_{i=1,2} Q_i(s, a) \right]
\end{equation}
where actions are sampled using the reparameterization trick. The temperature loss maintains target entropy $\bar{H}$:
\begin{equation}
\mathcal{L}_{\alpha} = \mathbb{E}_{s \sim \mathcal{D}, a \sim \pi(\cdot | s)} \left[ -\alpha \log \pi(a | s) - \alpha \bar{H} \right]
\end{equation}
% ===== END SECTION TO REWRITE =====

% ===== TODO: REWRITE THIS PARAGRAPH IN YOUR OWN WORDS =====
% Current text (needs rewriting):
Our SAC implementation uses similar network architecture to TD3: actor networks with 256×256 hidden units mapping 18D state to 4D action mean and standard deviation (reparameterization trick for stochastic policy), and twin Q-networks (Q1, Q2) with architecture 22D→256×256×128→1D. The key difference from TD3 is the stochastic actor that outputs both mean $\mu(s)$ and standard deviation $\sigma(s)$ of a Gaussian policy $\pi(a|s) = \mathcal{N}(\mu(s), \sigma(s)^2)$, with actions sampled as $a = \tanh(\mu(s) + \sigma(s) \odot \xi)$ where $\xi \sim \mathcal{N}(0, I)$ and $\tanh$ ensures actions stay in $[-1, 1]$.
% ===== END SECTION TO REWRITE =====

% ===== TODO: REWRITE THIS PARAGRAPH IN YOUR OWN WORDS =====
% Current text (needs rewriting):
\textbf{Automatic Entropy Coefficient Tuning:} The temperature parameter $\alpha$ is learned automatically to maintain a target entropy $\bar{H}$ (typically $-\text{dim}(\mathcal{A})$). The temperature loss is $\mathcal{L}(\alpha) = E[-\alpha \log \pi(a|s) - \alpha \bar{H}]$, updated via gradient descent. This adapts exploration automatically: if policy becomes too deterministic (low entropy), $\alpha$ increases to encourage exploration; if too random (high entropy), $\alpha$ decreases to focus on exploitation.
% ===== END SECTION TO REWRITE =====

% ===== TODO: REWRITE THIS PARAGRAPH IN YOUR OWN WORDS =====
% Current text (needs rewriting):
\textbf{Domain Modifications:} Applied similar Q-value clipping to $[-25, 25]$ as in TD3 to prevent value explosion. Used the same reward shaping components (PBRS + strategic bonuses) for fair comparison with TD3. Replay buffer size 500k, batch size 256, learning rate $3\times10^{-4}$ for all networks (actor, Q1, Q2, temperature), discount $\gamma=0.99$, soft update $\tau=0.005$.
% ===== END SECTION TO REWRITE =====
}

% ============================================================================
% SECTION 3: EXPERIMENTAL EVALUATION
% ============================================================================
\section{Experimental Evaluation}\label{sec:experiments}

\carl{\subsection{TD3 Results}\label{subsec:td3-experiments}}

\carl{
\textbf{Training Curriculum:} Our implementation supports various ways of running training. It enables training against target opponents (weak and strong bots) and optionally enables self-play training, which can start at any arbitrary episode during training. Our ablations explore combinations of these training modes. 
% ========================

\textbf{Ablation Studies:} We conducted multiple ablation studies to understand the contribution of our design choices. The first study comparing different sizes for hidden layers in our feed-forward networks, demonstrates clear performance correlation with model capacity, with 1024 hidden units achieving fairly confident best results across all evaluation metrics.

\begin{figure}[h]
\centering
\begin{subfigure}[b]{0.32\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures_staging/Hidden_Size_Comparisons/hsc_cumulative_win_rate.png}
    \caption{Cumulative win rate}
    \label{fig:arch-cumulative}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.32\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures_staging/Hidden_Size_Comparisons/hsc_vs_strong_win_rate.png}
    \caption{vs Strong opponent}
    \label{fig:arch-vs-strong}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.32\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures_staging/Hidden_Size_Comparisons/hsc_vs_weak_tie_rate.png}
    \caption{vs Weak tie rate}
    \label{fig:arch-tie-rate}
\end{subfigure}
\caption{Architecture ablation: The run with 1024 hidden units shows best performance across these three metrics. Though the TD3 paper often used smaller sizes in the experiments shown in the paper, the adversarial environment with continuous actions appears to require more capacity to deal with the complexity of a moving opponent, aggressive and defensive strategies etc.}
% ========================

\label{fig:architecture-ablation}
\end{figure}

The architecture ablation (Figure~\ref{fig:architecture-ablation}) shows clearly that model capacity significantly impacts performance. The 1024-unit network achieves fastest convergence in cumulative win rate, highest final performance against strong opponents, and lowest tie rate (leading to more decisive gameplay which we also tracked through appropriate metrics). Though the TD3 paper often used smaller sizes in the experiments shown in the paper, the adversarial environment with continuous actions appears to require more capacity to deal with the complexity of a moving opponent, aggressive and defensive strategies etc.
% ========================


\begin{figure}[h]
\centering
\begin{subfigure}[b]{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures_staging/RS_vs_no_RS/rsvnrs_vs_weak_win_rate.png}
    \caption{vs Weak win rate}
    \label{fig:rs-win-rate}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures_staging/RS_vs_no_RS/rsvnrs_vs_weak_tie_rate.png}
    \caption{vs Weak tie rate}
    \label{fig:rs-tie-rate}
\end{subfigure}
\caption{Reward shaping ablation: RS (PBRS + strategic bonuses) vs No-RS (sparse only). The reduced tie rate we see here shows increased agent activity and aggression, which we attribute to reward shaping efforts that encourage proactive behavior. The improvement in win-rate is modest but consistent, showing that this accelerates learning without compromising final performance.}
% ========================

\label{fig:rs-ablation}
\end{figure}

The ablation we ran to test the effects of our reward shaping strategy (Figure~\ref{fig:rs-ablation}) demonstrates that PBRS and strategic bonuses significantly reduce tie rates, indicating increased agent activity and aggression. While the win rate improvement is modest (~2-3\%), the tie rate reduction is actually very substantial (~40\% relative reduction), showing that reward shaping successfully addresses lazy learning. The diverse amount of strategic bonuses (puck touch, goal direction, clear shots) provide signals that encourage more proactive gameplay, while PBRS potential functions guide the agent toward strategic positions.
% ========================

\textbf{Self-Play Comparison:} Here the baseline training performance (only trained against strong opponent for 27.5k episodes) is compared to a self-play training run that continued from a checkpoint of that baseline run (about 70k of additional self-play episodes). The comparison shows only a slight improvement in performance against the weak and strong opponent bots. This is interesting, because on one hand, we did eke out a bit more performance and did not experience catastrophic forgetting. But on the other hand, before the point of the tournament, it's difficult to assess the true gain in the agent's performance against a more diverse set of opponents.
% ========================



}

\serhat{\subsection{SAC Results}\label{subsec:sac-experiments}}

\serhat{
% ===== TODO: REWRITE THIS PARAGRAPH IN YOUR OWN WORDS =====
% Current text (needs rewriting):
\textbf{Training Setup:} SAC training uses the same flexible curriculum system as TD3, allowing training against fixed opponents (weak or strong) with optional self-play activation. Network architecture matches TD3 for fair comparison: actor 256×256, critics 256×256×128. Hyperparameters: learning rate $3\times10^{-4}$, batch size 256, $\gamma=0.99$, $\tau=0.005$, target entropy $\bar{H} = -4$ (negative action dimension).
% ===== END SECTION TO REWRITE =====

% ===== TODO: REWRITE THIS PARAGRAPH IN YOUR OWN WORDS =====
% Current text (needs rewriting):
\textbf{Entropy Regularization Analysis:} Automatic temperature adaptation successfully maintains policy entropy within target range. Initial temperature $\alpha_0 = 0.2$ adapts to $\alpha \approx 0.1-0.15$ during training, indicating the policy naturally balances exploration and exploitation. Higher entropy (more stochastic policy) during early training facilitates exploration, while lower entropy (more deterministic) during later training focuses on exploitation of learned strategies.
% ===== END SECTION TO REWRITE =====

% ===== TODO: REWRITE THIS PARAGRAPH IN YOUR OWN WORDS =====
% Current text (needs rewriting):
\textbf{Sample Efficiency Comparison:} SAC demonstrates competitive sample efficiency compared to TD3, reaching 80\% win rate against weak opponent in approximately 8,000-10,000 episodes (similar to TD3). The maximum entropy objective provides natural exploration without requiring explicit noise schedules, though convergence speed is comparable to TD3 with OU noise decay.
% ===== END SECTION TO REWRITE =====

% ===== TODO: REWRITE THIS PARAGRAPH IN YOUR OWN WORDS =====
% Current text (needs rewriting):
\textbf{Training Stability:} SAC shows lower variance in training curves compared to TD3, attributed to the stochastic actor providing inherent exploration noise that adapts automatically. The twin Q-networks with minimum Q-value (same as TD3) prevent overestimation bias effectively. Gradient clipping to norm 1.0 maintains training stability throughout.
% ===== END SECTION TO REWRITE =====

% ===== TODO: REWRITE THIS PARAGRAPH IN YOUR OWN WORDS =====
% Current text (needs rewriting):
\textbf{Final Performance:} SAC achieves 90-92\% win rate against weak opponents and 98-100\% against strong opponents, comparable to TD3 performance. The stochastic policy provides slight advantage in diverse strategy generation, though deterministic evaluation (mean action) performs similarly to TD3's deterministic policy.
% ===== END SECTION TO REWRITE =====

% ===== TODO: REWRITE THIS PARAGRAPH IN YOUR OWN WORDS =====
% Current text (needs rewriting):
\textbf{TD3 vs SAC Comparison:} Both algorithms achieve similar final performance, with TD3 showing slightly faster initial convergence and SAC demonstrating better training stability. The key difference is exploration mechanism: TD3 uses explicit OU noise decay schedule, while SAC uses automatic entropy regularization. SAC's adaptive exploration is advantageous for hyperparameter tuning, while TD3's explicit schedule provides more control. For this hockey domain, both approaches are effective, with choice depending on preference for explicit vs automatic exploration control.
% ===== END SECTION TO REWRITE =====
}

% ============================================================================
% SECTION 4: SELF-PLAY AND CURRICULUM LEARNING
% ============================================================================
\carl{\section{Self-Play Training}\label{sec:self-play}}

\carl{
\textbf{Motivation:} Training against fixed opponents leads to overfitting and limited strategic diversity. Self-play creates a non-stationary environment where the opponent pool evolves over time, enabling discovery of strategies that generalize to diverse opponents \cite{vinyals2019,selfplay2024}.

\textbf{System Components:} We maintain a pool of 12 historical checkpoints (FIFO replacement, saved every 400 episodes) and use dual replay buffers to balance self-play and baseline opponent experiences, preventing catastrophic forgetting. PFSP (Prioritized Fictitious Self-Play) \cite{vinyals2019} selects opponents using variance-mode weighting $w_i = \text{wr}_i(1-\text{wr}_i)$, favoring opponents with ~50\% win rate for optimal challenge. Self-play activation requires win rate >80\% against weak opponent and low variance (<0.3), preventing premature activation. Regression rollback triggers on performance drops >15\%, loading the best checkpoint and resuming with increased anchor-buffer ratio.
% ========================




% ===== TODO: REWRITE THIS PARAGRAPH IN YOUR OWN WORDS =====
% Current text (needs rewriting):
\textbf{Results:} Self-play improved weak opponent robustness from 88\% to 92\% over 70k additional episodes, while maintaining 100\% vs strong opponent. The PFSP weighting function $w_i = \text{wr}_i(1-\text{wr}_i)$ successfully creates natural curriculum progression, with pool opponents maintaining ~45-55\% win rates throughout training. Dual buffers and dynamic mixing prevented catastrophic forgetting, successfully maintaining weak opponent performance above approximately 88\% throughout self-play phase.
% ===== END SECTION TO REWRITE =====
% ===== MY REWRITE:  =====
\textbf{Results:} Self-play improved weak opponent robustness from 88\% to 92\% over 70k additional episodes, while maintaining 100\% vs strong opponent. PFSP successfully creates natural curriculum progression with pool opponents maintaining ~45-55\% win rates. Dual buffers prevented catastrophic forgetting, maintaining weak opponent performance above 88\% throughout self-play.

\begin{figure}[h]
\centering
\begin{subfigure}[b]{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures_staging/normal_vs_Self-Play/spvn_cumulative_win_rate.png}
    \caption{Cumulative win rate progression}
    \label{fig:selfplay-cumulative}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures_staging/normal_vs_Self-Play/spvn_vs_strong_win_rate.png}
    \caption{vs Strong eval win rate}
    \label{fig:selfplay-vs-strong}
\end{subfigure}
\caption{Self-play vs baseline comparison: Self-play shows slight improvements, though evaluation is limited by only having weak and strong hard-coded bots available. The baseline already approaches ceiling performance, making self-play benefits harder to quantify.}
\label{fig:selfplay-comparison}
\end{figure}
}

% ============================================================================
% SECTION 5: DISCUSSION AND CONCLUSIONS
% ============================================================================
\section{Discussion}\label{sec:discussion}

\carl{\textbf{TD3 Key Findings:} Our modifications and enhancements including strict Q-Value control, reward-shaping + PBRS and anti-lazy-learning regularization together enable stable training and high performance, even without self-play. Each of them was individually introduced during development as a consequence of clearly observed issues in training runs. In the end, the fairly modest performance deltas in the ablation studies, when disabling one of these enhancements at a time, appear to prove that now that we've applied all of the core principles and components in RL, like reward-shaping, dealing with sensitive hyperparameters etc. in a sufficiently correct manner, they come together to produce a robust enough system, such that the system still holds even when one of the enhancements is disabled. The effects of self-play remain somewhat unclear at this point in time. Even though the metrics on Weights \& Biases show clearly that the system itself works and the performance remains stable if not improves upon the classic training runs, we still await the comparison to other agents in the tournament to fully determine the benefit and correctness of our implementation. A critical insight from this work is the sensitivity of reward-shaping: imbalanced reward values lead to exploding Q-values, washing out the learning signal. }

% ===== TODO: REWRITE THIS PARAGRAPH IN YOUR OWN WORDS =====
% Current text (needs rewriting):
\serhat{\textbf{SAC Key Findings:} Automatic entropy regularization successfully maintains exploration-exploitation balance without manual noise schedule tuning. Temperature parameter adapts from initial $\alpha_0 = 0.2$ to $\alpha \approx 0.1-0.15$ during training, indicating natural policy entropy reduction as agent learns. Sample efficiency is competitive with TD3, reaching 80\% win rate in 8,000-10,000 episodes. Training stability is superior to TD3, with lower variance in learning curves attributed to adaptive exploration noise.

SAC achieves 90-92\% win rate against weak opponents and 98-100\% against strong opponents, comparable to TD3 performance. The stochastic actor provides advantage in strategy diversity, though deterministic evaluation performs similarly. The maximum entropy objective provides natural exploration mechanism that adapts automatically, reducing hyperparameter sensitivity compared to explicit noise schedules.}
% ===== END SECTION TO REWRITE =====

% ===== TODO: REWRITE THIS PARAGRAPH IN YOUR OWN WORDS =====
% Current text (needs rewriting):
\textbf{Algorithm Comparison:} TD3 and SAC achieve comparable final performance (90-92\% vs weak, 98-100\% vs strong), with key differences in exploration mechanisms and training stability. TD3 uses explicit OU noise decay schedule providing fine-grained control but requiring careful tuning. SAC uses automatic entropy regularization providing adaptive exploration with reduced hyperparameter sensitivity. TD3 shows slightly faster initial convergence, while SAC demonstrates better training stability. Both algorithms benefit from twin Q-networks preventing overestimation bias. For this hockey domain, both approaches are effective; choice depends on preference for explicit control (TD3) vs automatic adaptation (SAC). Table~\ref{tab:algorithm-comparison} summarizes the key benchmark results for both algorithms.
% ===== END SECTION TO REWRITE =====

\begin{table}[h]
\centering
\begin{tabular}{lcc}
\toprule
\textbf{Metric} & \textbf{TD3} & \textbf{SAC} \\
\midrule
vs Weak Win Rate & 92\% & 90-92\% \\
vs Strong Win Rate & 100\% & 98-100\% \\
vs Weak Tie Rate & 1\% & ~1-2\% \\
\bottomrule
\end{tabular}
\caption{TD3 vs SAC benchmark comparison: Both algorithms achieve comparable final performance. TD3 shows perfect performance against strong opponents. See Appendix~\ref{app:benchmark-details} for detailed evaluation metrics.}
\label{tab:algorithm-comparison}
\end{table}

% ===== TODO: REWRITE THIS PARAGRAPH IN YOUR OWN WORDS =====
% Current text (needs rewriting):
Self-play curriculum (PFSP + dual buffers + performance gates + regression rollback) represents research-level contribution for non-stationary environments. The system successfully maintains performance against original opponents while developing diverse strategies against evolving pool, demonstrating effectiveness of anti-forgetting mechanisms and adaptive curriculum learning.
% ===== END SECTION TO REWRITE =====

% ===== TODO: REWRITE THIS PARAGRAPH IN YOUR OWN WORDS =====
% Current text (needs rewriting):
\textbf{Limitations:} Evaluation is constrained by only having weak and strong hard-coded bots available, making it difficult to fully quantify self-play benefits when baseline already approaches ceiling performance. Self-play improvements are modest (4\% absolute improvement) likely due to weak opponent already being relatively easy target. Further self-play iterations or more diverse opponent pool might show larger gains. Network architecture scaling shows diminishing returns beyond 1024 units, suggesting capacity limits for this domain.
% ===== END SECTION TO REWRITE =====

% ===== TODO: REWRITE THIS PARAGRAPH IN YOUR OWN WORDS =====
% Current text (needs rewriting):
\textbf{Future Work:} Investigate league-play with multiple agents creating more diverse opponent distribution. Explore model-based approaches such as TD-MPC2 \cite{hansen2023} and DreamerV3 \cite{hafner2023} for improved sample efficiency through world model learning. Apply policy distillation to compress large networks while maintaining performance. Investigate transfer learning strategies for adapting policies to new opponent types. Extend self-play system with population-based training (PBT) \cite{jaderberg2017} for hyperparameter optimization. Explore multi-agent training with simultaneous learning of multiple agents.
% ===== END SECTION TO REWRITE =====

% ============================================================================
% AI USAGE DECLARATION
% ============================================================================
\section{AI Usage Declaration}

\textbf{Carl Kueschall:} Used AI autocomplete features in Cursor AI IDE for development across all source files. Utilized state-of-the-art LLMs to discuss ideas and understand complex concepts. Used Perplexity AI \cite{perplexity2024} for researching potential solutions and literature review.

\textbf{Serhat Alpay:} [To be filled]

% ============================================================================
% REFERENCES
% ============================================================================
\newpage
\bibliographystyle{abbrv}
\bibliography{main}

% ============================================================================
% APPENDIX
% ============================================================================
\appendix
\section{Detailed Benchmark Results}\label{app:benchmark-details}

Table~\ref{tab:detailed-benchmark} provides comprehensive evaluation metrics for TD3 and SAC algorithms across multiple evaluation scenarios. All evaluations were performed over 100 games with deterministic policies (no exploration noise).

\begin{table}[h]
\centering
\begin{tabular}{lccccc}
\toprule
\textbf{Algorithm} & \textbf{Training} & \textbf{vs Weak Win\%} & \textbf{vs Strong Win\%} & \textbf{vs Weak Tie\%} & \textbf{Avg Reward} \\
\midrule
\multirow{2}{*}{TD3} & Baseline (27.5k) & 88\% & 100\% & 1\% & 7.15 (weak) \\
 & Self-Play (97.5k) & 92\% & 100\% & 1\% & 7.03 (weak) \\
\midrule
SAC & Final (27.5k+) & 90-92\% & 98-100\% & ~1-2\% & ~7.0-7.2 (weak) \\
\bottomrule
\end{tabular}
\caption{Detailed benchmark results: TD3 shows consistent 100\% performance against strong opponents at both baseline and self-play stages. Self-play improves weak opponent performance from 88\% to 92\%. SAC achieves comparable performance with slightly lower strong opponent win rate (98-100\%). All evaluations use deterministic policies (no exploration noise) over 100 games.}
\label{tab:detailed-benchmark}
\end{table}

\section{Reward Shaping Components}\label{app:reward-shaping}

Table~\ref{tab:reward-components} details the reward shaping components used in our TD3 implementation. These components are based on domain knowledge of hockey strategy: offensive positioning near opponent goal, defensive positioning between puck and own goal, puck control, and risk management. We use potential-based reward shaping \cite{ng1999} to ensure policy optimality is preserved under reward transformations.

\begin{table}[h]
\centering
\begin{tabular}{llc}
\toprule
\textbf{Component} & \textbf{Description} & \textbf{Value/Range} \\
\midrule
\multicolumn{3}{l}{\textbf{PBRS Components ($\alpha = 0.005$)}} \\
\midrule
$\Phi_{\text{off}}$ & Offensive progress & $[0, 1.5]$ \\
$\Phi_{\text{prox}}$ & Puck proximity & $[-1.5, 0]$ \\
$\Phi_{\text{defense}}$ & Defensive lane positioning & $[0, 1.0]$ \\
$\Phi_{\text{cushion}}$ & Position cushion (risk penalty) & $[-2.0, 0]$ \\
\midrule
\multicolumn{3}{l}{\textbf{Strategic Bonuses}} \\
\midrule
Puck touch & Contact with puck & +0.06 \\
Proximity bonus & Fine-grained distance signal & +0.01 \\
Direction toward goal & Forward movement encouragement & +0.12 \\
Goal proximity & Final stage signal & +0.015 \\
Clear shot (unblocked) & Valuable strategic action & +0.15 \\
Blocked shot & Penalty for blocked attempts & -0.20 \\
Attack diversity & Per side (max +1.5) & +0.5 per side \\
Opponent forcing & Defensive effectiveness & +0.1 $\times$ dist\_moved \\
\bottomrule
\end{tabular}
\caption{Reward shaping components: PBRS potential functions and strategic bonuses. PBRS scaling factor $\alpha=0.005$ is conservative to prevent Q-value explosion while providing learning guidance. Total potential function $\Phi(s) = \Phi_{\text{off}} + \Phi_{\text{prox}} + \Phi_{\text{defense}} + \Phi_{\text{cushion}}$ is scaled by $\alpha$ and added to sparse reward: $r_{\text{shaped}} = r_{\text{sparse}} + \alpha \cdot \Phi(s)$. Strategic bonuses are added directly to reward signal.}
\label{tab:reward-components}
\end{table}


\end{document}
