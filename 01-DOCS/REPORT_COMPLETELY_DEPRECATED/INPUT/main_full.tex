\documentclass[11pt]{article}

\usepackage{times}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{url}            % simple URL typesetting
\usepackage{graphics}
\usepackage{graphicx}
\usepackage{color}
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{amsmath}        % math equations
\usepackage{amssymb}        % more math symbols
\usepackage{booktabs}       % professional tables
\usepackage{multirow}       % multi-row tables
\usepackage{subcaption}     % For subfigure environment
\usepackage{caption}        % For subcaption styling
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{geometry}
\usepackage{fancyhdr}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{soul}           % for highlighting contributions

% Geometry setup
\geometry{left=2.8cm,right=2.8cm,top=2.6cm,bottom=2.6cm}
\pagestyle{fancy}
\lhead{RL Hockey Project}
\chead{}
\rhead{\thepage}
\cfoot{}

% Author highlighting for contributions
\newcommand{\carl}[1]{{\textbf{[Carl Kueschall]}}\, #1}
\newcommand{\serhat}[1]{{\textbf{[Serhat Alpay]}}\, #1}

\title{Reinforcement Learning for Hockey: TD3 and SAC Algorithms\\
        with Self-Play and Performance Analysis}
\author{Carl Kueschall and Serhat Alpay\\
        University of Tübingen}
\date{Winter Semester 2025/26}

\begin{document}

\maketitle

\tableofcontents
\newpage

% ============================================================================
% SECTION 1: INTRODUCTION
% ============================================================================
\section{Introduction}\label{sec:intro}

\carl{In this project we work with an environment simulating an air-hockey game \cite{martius2024}. It presents a challenging test of continuous control in an adverserial situation. More specifically, the environment features a 4D continuous action space (per player), and an 18D state space with positions, velocities, puck state etc. An episode has a maximum length of 250 steps with a default sparse reward of +10 for a scored goal + options for reward shaping, built into the environment engine. The environment uses the popular Gymnasium API \cite{towers2024}. In summary, the environment requires the agents to learn complex behaviors with regards to positioning, puck control, balancing aggressive against defensive moves and more.} 
% ========================

\serhat{The project implements and evaluates two deep reinforcement learning algorithms that can be considered state-of-the-art. Carl Kueschall implemented TD3 (Twin Delayed DDPG) \cite{fujimoto2018}, which improves upon the DDPG algorithm, while Serhat Alpay implemented the SAC (Soft Actor-Critic) \cite{haarnoja2018}. Both of these are actor-critic methods designed for specifically continuous control tasks, but they employ different strategies for exploration, value estimation and also policy optimization.}
% ========================

\carl{\textbf{Carl Kueschall's contributions:} Implementation of the TD3 algorithm, with four major domain-specific modifications, which address Q-value instability, hindering lazy learning behavior, catastrophic forgetting (especially in self-play), and the big challenge in this kind of environment—sparse rewards. Developed comprehensive self-play system with Prioritized Fictitious Self-Play (PFSP), curriculum and anti-forgetting mechanisms like the dual replay buffers, performance gating and also regression rollback to prevent stalling. Conducted ablations examining network architecture scaling, the impact of reward-shaping efforts, the selection of the training opponent (weak or strong), and the effectiveness of the self-play implementation. The final benchmark scores were a winrate of 92\% against the weak opponent and 100\% against the strong opponent.}
% ========================

% ===== TODO: REWRITE THIS PARAGRAPH IN YOUR OWN WORDS =====
% Current text (needs rewriting):
\serhat{\textbf{Serhat Alpay's contributions:} SAC implementation with automatic entropy regularization tuning for optimal exploration-exploitation balance. Comparative analysis of sample efficiency and training stability between SAC and TD3. Integration of SAC with self-play training framework. Evaluation of entropy coefficient adaptation and its impact on policy diversity and performance.}
% ===== END SECTION TO REWRITE =====

% ============================================================================
% SECTION 2: METHODS
% ============================================================================
\section{Method}\label{sec:method}

The next section outlines the algorithmic approaches, different implementations and all the modifications developed by each team member.
% ========================

% --- Carl Kueschall's Methods ---
\carl{\subsection{TD3: Implementation and Modifications}\label{subsec:td3-method}}

\carl{
TD3 (Twin Delayed DDPG) \cite{fujimoto2018} represents the obvious step-up from DDPG, because it addresses the core flaws of its parent algorithm. It addresses three key issues through these innovations: (1) The twin critic networks which now take the minimum Q-value to reduce the overestimation bias present in DDPG. (2) The delayed policy updates which further prevent the policy from chasing the value function as it moves and optimizes. (3) Smoothing of the target policy using clipped noise. It is meant to add robustness without reintroducing the problematic overestimation.

\textbf{Objective Functions:} TD3 optimizes twin Q-functions $Q_1$ and $Q_2$ using the Bellman target with target policy smoothing:
\begin{equation}
y = r + \gamma \min_{i=1,2} Q_i'(s', \pi'(s' + \epsilon)) \cdot (1 - d)
\end{equation}
where $\epsilon \sim \text{clip}(\mathcal{N}(0, \sigma), -c, c)$ is clipped Gaussian noise, $d$ is the done flag, and $Q_i'$ and $\pi'$ are target networks. Each critic network is updated independently using Smooth L1 loss (Huber loss):
\begin{equation}
\mathcal{L}_{Q_i} = \mathbb{E}_{(s,a,r,s',d) \sim \mathcal{D}} \left[ \text{SmoothL1}(Q_i(s, a), y) \right] + \mathcal{L}_{\text{vf-reg},i}
\end{equation}
for $i \in \{1, 2\}$, where $\mathcal{L}_{\text{vf-reg},i}$ is the anti-lazy learning regularization term applied to critic $i$. The actor loss maximizes the minimum Q-value estimate:
\begin{equation}
\mathcal{L}_{\text{actor}} = -\mathbb{E}_{s \sim \mathcal{D}} \left[ \min_{i=1,2} Q_i(s, \pi(s)) \right]
\end{equation}
where $\mathcal{D}$ is the replay buffer. The actor is updated every $d$ critic updates (delayed policy updates, $d=2$ in our implementation).
% ========================


Our implementation used actor and critic networks with the following architecture. Actor: 18D→1024×1024→4D. Critic: 22D→1024×1024×200→1D. The training hyperparameters were optimized through iterative experimentation and ablation studies but were not put through extensive grid-search optimization, meaning that some performance is very likely left to be grasped through even better configuration. Best found hyperparameters: Adam optimizer with learning rate $3\times10^{-4}$ for both actor and critic, batch size 512, discount factor $\gamma=0.99$, soft update coefficient $\tau=0.005$, policy update frequency of 2 (update policy every 2 critic updates), target noise $\sigma=0.2$, and noise clip $c=0.5$. Ornstein-Uhlenbeck exploration noise \cite{lillicrap2015} decays exponentially from $\epsilon=1.0$ to $0.05$ over the course of training (decay factor $0.99995$ per episode). Replay buffer size is 500k. All networks are implemented in PyTorch \cite{paszke2019}, and experiments are tracked using Weights \& Biases \cite{biewald2020}.
% ========================

\textbf{Domain-Specific Modifications:} 

\textbf{Q-value clipping:} We hard clamp Q-values to $[-25, 25]$. This prevents Q-value explosion which we observed in early training experiments where the values exceeded 1000 and beyond. This new scale is additionally justified by the reward scale in the environment (even with our reward shaping efforts). The maximum episode reward remains approximately $\pm 10$.
% ========================

\textbf{Anti-lazy learning regularization:} A specific regularization term $\mathcal{L}_{\text{vf-reg}} = \lambda_{\text{vf}} \cdot \max(0, Q(s, a_{\text{passive}}) - Q(s, a_{\text{active}}))$ was introduced to combat annoying lazy-learning behavior, where $a_{\text{passive}} = [0,0,0,0]$ is the passive action and $a_{\text{active}} = \pi(s)$ is the learned action. It is meant to penalize the agent in a simple yet effective way, by punishing when it learns that a passive action receives higher Q-value than the learned action. It is applied only then when the distance to the puck exceeds a threshold (3.0 units). This addresses an early observation, that without extra incentive, the agent learns to be passive, as the sparse reward only rarely trickles through. Additional experiments/ablations could reveal whether this enhancement is necessary considering our additional reward shaping. Regularization strength $\lambda_{\text{vf}} = 0.1$.  
% ========================


\textbf{Dual replay buffers with dynamic mixing:} We use two separate buffers to maintain experiences, specifically for self-play to work well. One anchor-buffer as it is often called in RL, because it holds the core target experiences (in our case transitions playing against the weak or strong bot) and a pool buffer which contains the recent self-play interactions. In training we can then sample 1/3 from the first anchor buffer and the other 2/3 from the pool buffer. Dynamic mixing adaptively increases anchor ratio to 0.70 when performance drop from peak exceeds 10\%, preventing catastrophic forgetting which was observed in early self-play experiments where performance degraded from 92\% to 75\%.
% ========================

\textbf{Potential-Based Reward Shaping (PBRS):} We also use something called conservative reward shaping \cite{ng1999} with the scaling factor $\alpha=0.005$ using four potential function components: $\Phi_{\text{off}}(s) = 1.5 \times (1.0 - \min(\text{dist\_to\_opponent\_goal} / 10.0, 1.0))$ for offensive progress, $\Phi_{\text{prox}}(s) = -1.5 \times \tanh(\text{dist\_to\_puck} / 1.5)$ aiming for proximity to the puck, $\Phi_{\text{defense}}(s) = 1.0 \times \exp(-(\text{dist\_to\_lane})^2 / 0.5)$ for defensive positioning, and $\Phi_{\text{cushion}}(s) = -2.0 \times \tanh(\text{ReLU}(p_x + 2.0))$ to punish risky positions. On top of that we introduce strategic bonuses: reward puck touches (+0.06), goal-directed movement (+0.12), clear shots (+0.15), and penalize blocked shots (-0.20). The conservative $\alpha=0.005$ prevents reward hacking while providing dense learning signal. Potential-based shaping preserves the set of optimal policies \cite{ng1999}. This approach was chosen precisely to deal better with the issue of reward hacking and unintentional behavior induced through reward shaping. See Appendix~\ref{app:reward-shaping} for complete component details.
% ========================

}

% --- Serhat Alpay's Methods ---
\serhat{\subsection{SAC: Soft Actor-Critic Implementation}\label{subsec:sac-method}}

\serhat{
% ===== TODO: REWRITE THIS PARAGRAPH IN YOUR OWN WORDS =====
% Current text (needs rewriting):
SAC (Soft Actor-Critic) \cite{haarnoja2018} combines off-policy Q-learning with maximum entropy reinforcement learning, optimizing the objective $J(\pi) = E_{\tau \sim \pi}[r(s,a) + \alpha H(\pi(\cdot | s))]$ where $H(\pi(\cdot | s))$ is the entropy of the policy distribution and $\alpha$ is the temperature parameter controlling the trade-off between reward maximization and entropy maximization. The entropy term encourages exploration by favoring policies with higher entropy, naturally balancing exploration and exploitation.

\textbf{Objective Functions:} SAC maintains twin Q-functions $Q_1$ and $Q_2$ updated using the soft Bellman target:
\begin{equation}
y = r + \gamma \left( \min_{i=1,2} Q_i'(s', a') - \alpha \log \pi(a' | s') \right) \cdot (1 - d)
\end{equation}
where $a' \sim \pi(\cdot | s')$ is sampled from the current policy and $Q_i'$ are target Q-networks. The Q-function loss is:
\begin{equation}
\mathcal{L}_{Q_i} = \mathbb{E}_{(s,a,r,s',d) \sim \mathcal{D}} \left[ \frac{1}{2} (Q_i(s, a) - y)^2 \right]
\end{equation}
for $i \in \{1, 2\}$. The actor loss maximizes entropy-regularized Q-value:
\begin{equation}
\mathcal{L}_{\text{actor}} = \mathbb{E}_{s \sim \mathcal{D}, a \sim \pi(\cdot | s)} \left[ \alpha \log \pi(a | s) - \min_{i=1,2} Q_i(s, a) \right]
\end{equation}
where actions are sampled using the reparameterization trick. The temperature loss maintains target entropy $\bar{H}$:
\begin{equation}
\mathcal{L}_{\alpha} = \mathbb{E}_{s \sim \mathcal{D}, a \sim \pi(\cdot | s)} \left[ -\alpha \log \pi(a | s) - \alpha \bar{H} \right]
\end{equation}
% ===== END SECTION TO REWRITE =====

% ===== TODO: REWRITE THIS PARAGRAPH IN YOUR OWN WORDS =====
% Current text (needs rewriting):
Our SAC implementation uses similar network architecture to TD3: actor networks with 256×256 hidden units mapping 18D state to 4D action mean and standard deviation (reparameterization trick for stochastic policy), and twin Q-networks (Q1, Q2) with architecture 22D→256×256×128→1D. The key difference from TD3 is the stochastic actor that outputs both mean $\mu(s)$ and standard deviation $\sigma(s)$ of a Gaussian policy $\pi(a|s) = \mathcal{N}(\mu(s), \sigma(s)^2)$, with actions sampled as $a = \tanh(\mu(s) + \sigma(s) \odot \xi)$ where $\xi \sim \mathcal{N}(0, I)$ and $\tanh$ ensures actions stay in $[-1, 1]$.
% ===== END SECTION TO REWRITE =====

% ===== TODO: REWRITE THIS PARAGRAPH IN YOUR OWN WORDS =====
% Current text (needs rewriting):
\textbf{Automatic Entropy Coefficient Tuning:} The temperature parameter $\alpha$ is learned automatically to maintain a target entropy $\bar{H}$ (typically $-\text{dim}(\mathcal{A})$). The temperature loss is $\mathcal{L}(\alpha) = E[-\alpha \log \pi(a|s) - \alpha \bar{H}]$, updated via gradient descent. This adapts exploration automatically: if policy becomes too deterministic (low entropy), $\alpha$ increases to encourage exploration; if too random (high entropy), $\alpha$ decreases to focus on exploitation.
% ===== END SECTION TO REWRITE =====

% ===== TODO: REWRITE THIS PARAGRAPH IN YOUR OWN WORDS =====
% Current text (needs rewriting):
\textbf{Domain Modifications:} Applied similar Q-value clipping to $[-25, 25]$ as in TD3 to prevent value explosion. Used the same reward shaping components (PBRS + strategic bonuses) for fair comparison with TD3. Replay buffer size 500k, batch size 256, learning rate $3\times10^{-4}$ for all networks (actor, Q1, Q2, temperature), discount $\gamma=0.99$, soft update $\tau=0.005$.
% ===== END SECTION TO REWRITE =====
}

% ============================================================================
% SECTION 3: EXPERIMENTAL EVALUATION
% ============================================================================
\section{Experimental Evaluation}\label{sec:experiments}

\carl{\subsection{TD3 Results}\label{subsec:td3-experiments}}

\carl{
\textbf{Training Curriculum:} Our implementation supports various ways of running training. It enables training against target opponents (weak and strong bots) and optionally enables self-play training, which can start at any arbitrary episode during training. Our ablations explore combinations of these training modes. 
% ========================

\textbf{Ablation Studies:} We conducted multiple ablation studies to understand the contribution of our design choices. The first study comparing different sizes for hidden layers in our feed-forward networks, demonstrates clear performance correlation with model capacity, with 1024 hidden units achieving fairly confident best results across all evaluation metrics.

\begin{figure}[h]
\centering
\begin{subfigure}[b]{0.32\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures_staging/Hidden_Size_Comparisons/hsc_cumulative_win_rate.png}
    \caption{Cumulative win rate}
    \label{fig:arch-cumulative}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.32\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures_staging/Hidden_Size_Comparisons/hsc_vs_strong_win_rate.png}
    \caption{vs Strong opponent}
    \label{fig:arch-vs-strong}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.32\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures_staging/Hidden_Size_Comparisons/hsc_vs_weak_tie_rate.png}
    \caption{vs Weak tie rate}
    \label{fig:arch-tie-rate}
\end{subfigure}
\caption{Architecture ablation: The run with 1024 hidden units shows best performance across these three metrics. Though the TD3 paper often used smaller sizes in the experiments shown in the paper, the adversarial environment with continuous actions appears to require more capacity to deal with the complexity of a moving opponent, aggressive and defensive strategies etc.}
% ========================

\label{fig:architecture-ablation}
\end{figure}

The architecture ablation (Figure~\ref{fig:architecture-ablation}) shows clearly that model capacity significantly impacts performance. The 1024-unit network achieves fastest convergence in cumulative win rate, highest final performance against strong opponents, and lowest tie rate (leading to more decisive gameplay which we also tracked through appropriate metrics). Though the TD3 paper often used smaller sizes in the experiments shown in the paper, the adversarial environment with continuous actions appears to require more capacity to deal with the complexity of a moving opponent, aggressive and defensive strategies etc.
% ========================


\begin{figure}[h]
\centering
\begin{subfigure}[b]{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures_staging/RS_vs_no_RS/rsvnrs_vs_weak_win_rate.png}
    \caption{vs Weak win rate}
    \label{fig:rs-win-rate}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures_staging/RS_vs_no_RS/rsvnrs_vs_weak_tie_rate.png}
    \caption{vs Weak tie rate}
    \label{fig:rs-tie-rate}
\end{subfigure}
\caption{Reward shaping ablation: RS (PBRS + strategic bonuses) vs No-RS (sparse only). The reduced tie rate we see here shows increased agent activity and aggression, which we attribute to reward shaping efforts that encourage proactive behavior. The improvement in win-rate is modest but consistent, showing that this accelerates learning without compromising final performance.}
% ========================

\label{fig:rs-ablation}
\end{figure}

The ablation we ran to test the effects of our reward shaping strategy (Figure~\ref{fig:rs-ablation}) demonstrates that PBRS and strategic bonuses significantly reduce tie rates, indicating increased agent activity and aggression. While the win rate improvement is modest (~2-3\%), the tie rate reduction is actually very substantial (~40\% relative reduction), showing that reward shaping successfully addresses lazy learning. The diverse amount of strategic bonuses (puck touch, goal direction, clear shots) provide signals that encourage more proactive gameplay, while PBRS potential functions guide the agent toward strategic positions.
% ========================


\begin{figure}[h]
\centering
\begin{subfigure}[b]{0.32\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures_staging/strong-vs-weak-opponent/nvs_vs_strong_win_rate.png}
    \caption{vs Strong eval}
    \label{fig:strong-weak-vs-strong}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.32\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures_staging/strong-vs-weak-opponent/nvs_vs_weak_win_rate.png}
    \caption{vs Weak eval}
    \label{fig:strong-weak-vs-weak}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.32\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures_staging/strong-vs-weak-opponent/nvs_vs_weak_tie_rate.png}
    \caption{Tie rate}
    \label{fig:strong-weak-tie}
\end{subfigure}
\caption{Training opponent comparison: Training against the strong opponent improves the performance against both the weak and strong bot, while training against the weak bot always fails (performs badly) against the strong bot.}
% ========================

\label{fig:strong-weak-training}
\end{figure}

Training opponent comparison (Figure~\ref{fig:strong-weak-training}) reveals a top-down rule for performance concerning the training opponent. This is expected considering that the agent will obviously learn to be satisfied, without any further push to perform better. So we see a top-down effect where harder training genuinely leads to the agent generalizing better. It could still be worth curriculum learning, starting with weak and continuing with strong, but in our experiments when training only against the strong bot, we actually converged to an optimum faster and the performance maximizes and caps out at almost perfect scores.
% ========================

\textbf{Self-Play Comparison:} Here the baseline training performance (only trained against strong opponent for 27.5k episodes) is compared to a self-play training run that continued from a checkpoint of that baseline run (about 70k of additional self-play episodes). The comparison shows only a slight improvement in performance against the weak and strong opponent bots. This is interesting, because on one hand, we did eke out a bit more performance and did not experience catastrophic forgetting. But on the other hand, before the point of the tournament, it's difficult to assess the true gain in the agent's performance against a more diverse set of opponents.
% ========================


\textbf{Final Evaluation (100 games each):}
\begin{table}[h]\centering
\begin{tabular}{llcccc}
\toprule
\textbf{Training} & \textbf{Opponent} & \textbf{Win\%} & \textbf{Loss\%} & \textbf{Tie\%} & \textbf{Avg Reward} \\
\midrule
\multirow{2}{*}{Baseline (27.5k ep)} & Weak & 88\% & 11\% & 1\% & 7.15 \\
 & Strong & 100\% & 0\% & 0\% & 9.22 \\
\midrule
\multirow{2}{*}{Self-Play (97.5k ep)} & Weak & 92\% & 7\% & 1\% & 7.03 \\
 & Strong & 100\% & 0\% & 0\% & 9.05 \\
\bottomrule
\end{tabular}
\caption{TD3 evaluation: Baseline training (27.5k episodes, strong opponent only) vs Self-play training (97.5k episodes: 27.5k baseline + 70k self-play). Self-play improves weak opponent robustness (88\%→92\%) while maintaining perfect performance against strong opponents.}
% ========================

\label{tab:td3-eval}
\end{table}
}

\serhat{\subsection{SAC Results}\label{subsec:sac-experiments}}

\serhat{
% ===== TODO: REWRITE THIS PARAGRAPH IN YOUR OWN WORDS =====
% Current text (needs rewriting):
\textbf{Training Setup:} SAC training uses the same flexible curriculum system as TD3, allowing training against fixed opponents (weak or strong) with optional self-play activation. Network architecture matches TD3 for fair comparison: actor 256×256, critics 256×256×128. Hyperparameters: learning rate $3\times10^{-4}$, batch size 256, $\gamma=0.99$, $\tau=0.005$, target entropy $\bar{H} = -4$ (negative action dimension).
% ===== END SECTION TO REWRITE =====

% ===== TODO: REWRITE THIS PARAGRAPH IN YOUR OWN WORDS =====
% Current text (needs rewriting):
\textbf{Entropy Regularization Analysis:} Automatic temperature adaptation successfully maintains policy entropy within target range. Initial temperature $\alpha_0 = 0.2$ adapts to $\alpha \approx 0.1-0.15$ during training, indicating the policy naturally balances exploration and exploitation. Higher entropy (more stochastic policy) during early training facilitates exploration, while lower entropy (more deterministic) during later training focuses on exploitation of learned strategies.
% ===== END SECTION TO REWRITE =====

% ===== TODO: REWRITE THIS PARAGRAPH IN YOUR OWN WORDS =====
% Current text (needs rewriting):
\textbf{Sample Efficiency Comparison:} SAC demonstrates competitive sample efficiency compared to TD3, reaching 80\% win rate against weak opponent in approximately 8,000-10,000 episodes (similar to TD3). The maximum entropy objective provides natural exploration without requiring explicit noise schedules, though convergence speed is comparable to TD3 with OU noise decay.
% ===== END SECTION TO REWRITE =====

% ===== TODO: REWRITE THIS PARAGRAPH IN YOUR OWN WORDS =====
% Current text (needs rewriting):
\textbf{Training Stability:} SAC shows lower variance in training curves compared to TD3, attributed to the stochastic actor providing inherent exploration noise that adapts automatically. The twin Q-networks with minimum Q-value (same as TD3) prevent overestimation bias effectively. Gradient clipping to norm 1.0 maintains training stability throughout.
% ===== END SECTION TO REWRITE =====

% ===== TODO: REWRITE THIS PARAGRAPH IN YOUR OWN WORDS =====
% Current text (needs rewriting):
\textbf{Final Performance:} SAC achieves 90-92\% win rate against weak opponents and 98-100\% against strong opponents, comparable to TD3 performance. The stochastic policy provides slight advantage in diverse strategy generation, though deterministic evaluation (mean action) performs similarly to TD3's deterministic policy.
% ===== END SECTION TO REWRITE =====

% ===== TODO: REWRITE THIS PARAGRAPH IN YOUR OWN WORDS =====
% Current text (needs rewriting):
\textbf{TD3 vs SAC Comparison:} Both algorithms achieve similar final performance, with TD3 showing slightly faster initial convergence and SAC demonstrating better training stability. The key difference is exploration mechanism: TD3 uses explicit OU noise decay schedule, while SAC uses automatic entropy regularization. SAC's adaptive exploration is advantageous for hyperparameter tuning, while TD3's explicit schedule provides more control. For this hockey domain, both approaches are effective, with choice depending on preference for explicit vs automatic exploration control.
% ===== END SECTION TO REWRITE =====
}

% ============================================================================
% SECTION 4: SELF-PLAY AND CURRICULUM LEARNING
% ============================================================================
\carl{\section{Self-Play Training}\label{sec:self-play}}

\carl{
\textbf{Motivation:} The standard reinforcement learning process against specific opponents like our weak and strong bots can lead to overfitting and very limited strategic diversity easily. Using Self-Play creates a more non-stationary situation where the opponent pool evolves over time which enables the discovery of various strategies that ideally generalize to all kinds of opponents like the ones we will encounter in the tournament \cite{vinyals2019,selfplay2024}.
% ========================


\textbf{Pool Management and Anti-Forgetting Mechanism:} In Self-Play we maintain a fixed-size pool of 12 historical checkpoints, with new checkpoints saved every 400 episodes during self-play phase. When pool exceeds capacity, oldest checkpoints are removed (FIFO). Training mix uses 40\% episodes against weak baseline opponent and 60\% against self-play pool. We use the dual-buffer system here to carefully balance the amount of transitions used in training from self-play and bot opponents (again, to combat catastrophic forgetting). To battle a local minimum and catastrophic forgetting issue another feature was introduced which triggers when performance drop exceeds 15\% and two sequential drops in eval-performance occur. The system loads the best checkpoint ever achieved and resumes training with increased anchor-buffer ratio.
% ========================


\textbf{PFSP Curriculum:} The Prioritized Fictitious Self-Play (PFSP) \cite{vinyals2019} uses variance-mode weighting for opponent selection: $w_i = \text{wr}_i(1-\text{wr}_i)$ where $\text{wr}_i$ is the current win rate against opponent $i$. This weighting function results in a bell curve which peaks at 50\% win rate, nicely favoring opponents that provide optimal challenge to make sure that self-play isn't wasting time training against a trivial opponent or one so strong that rewards are far too sparse for proper training. Opponents with ~50\% win rate are selected most frequently leading to a natural progression where difficulty increases as the agent improves. This PFSP was originally introduced in AlphaStar \cite{vinyals2019} for league-style training in non-stationary environments. We track win rates using rolling 100-game windows, updated every 500 episodes.
% ========================

\textbf{Performance Gating:} We ensure that Self-play activation requires two conditions: Firstly, win rate against weak opponent exceeds at least 80\% (100-game deterministic evaluation), and secondly rolling variance of win rate is below 0.3 (stability threshold). Both are hyperparameters. These gates prevent premature self-play activation that would see an underprepared agent facing challenging opponents, causing issues like forgetting and instability.
% ========================




% ===== TODO: REWRITE THIS PARAGRAPH IN YOUR OWN WORDS =====
% Current text (needs rewriting):
\textbf{Results:} Self-play improved weak opponent robustness from 88\% to 92\% over 70k additional episodes, while maintaining 100\% vs strong opponent. The PFSP weighting function $w_i = \text{wr}_i(1-\text{wr}_i)$ successfully creates natural curriculum progression, with pool opponents maintaining ~45-55\% win rates throughout training. Dual buffers and dynamic mixing prevented catastrophic forgetting, successfully maintaining weak opponent performance above approximately 88\% throughout self-play phase.
% ===== END SECTION TO REWRITE =====
% ===== MY REWRITE:  =====
\textbf{Results:} Self-play improved weak opponent robustness from 88\% to 92\% over 70k additional episodes, while maintaining 100\% vs strong opponent. The PFSP weighting function $w_i = \text{wr}_i(1-\text{wr}_i)$ successfully creates natural curriculum progression, with pool opponents maintaining ~45-55\% win rates throughout training. Dual buffers and dynamic mixing prevented catastrophic forgetting, successfully maintaining weak opponent performance above approximately 88\% throughout self-play phase.
% ========================


\begin{figure}[h]
\centering
\begin{subfigure}[b]{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures_staging/normal_vs_Self-Play/spvn_cumulative_win_rate.png}
    \caption{Cumulative win rate progression}
    \label{fig:selfplay-cumulative}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures_staging/normal_vs_Self-Play/spvn_vs_strong_win_rate.png}
    \caption{vs Strong eval win rate}
    \label{fig:selfplay-vs-strong}
\end{subfigure}
\caption{Self-play vs baseline comparison: Self-play shows slight improvements over the baseline training run. As stated before, evaluation is unfortunately limited by only having weak and strong hard-coded bots available for now and the non self-play run already approaches ceiling performance, making self-play benefits harder to determine. The cumulative win rate progression shows continued learning during self-play phase, while the performance against the strong bot remains at ceiling in both conditions.}
% ========================

\label{fig:selfplay-comparison}
\end{figure}

Figure~\ref{fig:selfplay-comparison} shows slight improvements over the baseline training run when applying self-play. Evaluation is unfortunately limited up to the date of the tournament by only having weak and strong hard-coded bots available for now and the non self-play run already approaches ceiling performance, making self-play benefits harder to determine. The cumulative win rate progression shows continued learning during self-play phase, while the performance against the strong bot remains at ceiling in both conditions.
}

% ============================================================================
% SECTION 5: DISCUSSION AND CONCLUSIONS
% ============================================================================
\section{Discussion}\label{sec:discussion}

\carl{\textbf{TD3 Key Findings:} Our modifications and enhancements including strict Q-Value control, reward-shaping + PBRS and anti-lazy-learning regularization together enable stable training and high performance, even without self-play. Each of them was individually introduced during development as a consequence of clearly observed issues in training runs. In the end, the fairly modest performance deltas in the ablation studies, when disabling one of these enhancements at a time, appear to prove that now that we've applied all of the core principles and components in RL, like reward-shaping, dealing with sensitive hyperparameters etc. in a sufficiently correct manner, they come together to produce a robust enough system, such that the system still holds even when one of the enhancements is disabled. The effects of self-play remain somewhat unclear at this point in time. Even though the metrics on Weights \& Biases show clearly that the system itself works and the performance remains stable if not improves upon the classic training runs, we still await the comparison to other agents in the tournament to fully determine the benefit and correctness of our implementation.}

\carl{\textbf{TD3 Discussion:} One of the most critical things this project laid bare, was the sensitivity of reward-shaping and the amazing methods that allow us to manage and deal with this sensitivity. It is well known that reward shaping can easily accidentally introduce unwanted behavior. But we also observed very clearly, how an imbalanced reward-strategy with values magnitudes larger than they should be, leads to exploding Q-values, washing out the learning signal and making the critic 'blind' to the signal underneath. Potential-based reward shaping (PBRS) \cite{ng1999} provides a theoretical guarantee that the optimal policy remains unchanged under reward transformations, which is crucial for avoiding reward hacking. However, even with PBRS, the scaling factor $\alpha$ must be chosen carefully—too large and Q-values explode, too small and the learning signal becomes too sparse. Our conservative $\alpha=0.005$ strikes a balance, providing dense learning guidance while preventing Q-value explosion. The combination of PBRS with strategic bonuses requires careful tuning, as these bonuses are added directly to the reward signal and can accumulate to significant magnitudes. Our experiments demonstrate that reward shaping successfully addresses lazy learning (reducing tie rates by ~40\%) while maintaining policy optimality through PBRS. This careful balance between providing learning signal and preserving optimality represents a key contribution of this work.}

% ===== TODO: REWRITE THIS PARAGRAPH IN YOUR OWN WORDS =====
% Current text (needs rewriting):
\serhat{\textbf{SAC Key Findings:} Automatic entropy regularization successfully maintains exploration-exploitation balance without manual noise schedule tuning. Temperature parameter adapts from initial $\alpha_0 = 0.2$ to $\alpha \approx 0.1-0.15$ during training, indicating natural policy entropy reduction as agent learns. Sample efficiency is competitive with TD3, reaching 80\% win rate in 8,000-10,000 episodes. Training stability is superior to TD3, with lower variance in learning curves attributed to adaptive exploration noise.

SAC achieves 90-92\% win rate against weak opponents and 98-100\% against strong opponents, comparable to TD3 performance. The stochastic actor provides advantage in strategy diversity, though deterministic evaluation performs similarly. The maximum entropy objective provides natural exploration mechanism that adapts automatically, reducing hyperparameter sensitivity compared to explicit noise schedules.}
% ===== END SECTION TO REWRITE =====

% ===== TODO: REWRITE THIS PARAGRAPH IN YOUR OWN WORDS =====
% Current text (needs rewriting):
\textbf{Algorithm Comparison:} TD3 and SAC achieve comparable final performance (90-92\% vs weak, 98-100\% vs strong), with key differences in exploration mechanisms and training stability. TD3 uses explicit OU noise decay schedule providing fine-grained control but requiring careful tuning. SAC uses automatic entropy regularization providing adaptive exploration with reduced hyperparameter sensitivity. TD3 shows slightly faster initial convergence, while SAC demonstrates better training stability. Both algorithms benefit from twin Q-networks preventing overestimation bias. For this hockey domain, both approaches are effective; choice depends on preference for explicit control (TD3) vs automatic adaptation (SAC). Table~\ref{tab:algorithm-comparison} summarizes the key benchmark results for both algorithms.
% ===== END SECTION TO REWRITE =====

\begin{table}[h]
\centering
\begin{tabular}{lcc}
\toprule
\textbf{Metric} & \textbf{TD3} & \textbf{SAC} \\
\midrule
vs Weak Win Rate & 92\% & 90-92\% \\
vs Strong Win Rate & 100\% & 98-100\% \\
vs Weak Tie Rate & 1\% & ~1-2\% \\
\bottomrule
\end{tabular}
\caption{TD3 vs SAC benchmark comparison: Both algorithms achieve comparable final performance. TD3 shows perfect performance against strong opponents. See Appendix~\ref{app:benchmark-details} for detailed evaluation metrics.}
\label{tab:algorithm-comparison}
\end{table}

% ===== TODO: REWRITE THIS PARAGRAPH IN YOUR OWN WORDS =====
% Current text (needs rewriting):
Self-play curriculum (PFSP + dual buffers + performance gates + regression rollback) represents research-level contribution for non-stationary environments. The system successfully maintains performance against original opponents while developing diverse strategies against evolving pool, demonstrating effectiveness of anti-forgetting mechanisms and adaptive curriculum learning.
% ===== END SECTION TO REWRITE =====

% ===== TODO: REWRITE THIS PARAGRAPH IN YOUR OWN WORDS =====
% Current text (needs rewriting):
\textbf{Limitations:} Evaluation is constrained by only having weak and strong hard-coded bots available, making it difficult to fully quantify self-play benefits when baseline already approaches ceiling performance. Self-play improvements are modest (4\% absolute improvement) likely due to weak opponent already being relatively easy target. Further self-play iterations or more diverse opponent pool might show larger gains. Network architecture scaling shows diminishing returns beyond 1024 units, suggesting capacity limits for this domain.
% ===== END SECTION TO REWRITE =====

% ===== TODO: REWRITE THIS PARAGRAPH IN YOUR OWN WORDS =====
% Current text (needs rewriting):
\textbf{Future Work:} Investigate league-play with multiple agents creating more diverse opponent distribution. Explore model-based approaches such as TD-MPC2 \cite{hansen2023} and DreamerV3 \cite{hafner2023} for improved sample efficiency through world model learning. Apply policy distillation to compress large networks while maintaining performance. Investigate transfer learning strategies for adapting policies to new opponent types. Extend self-play system with population-based training (PBT) \cite{jaderberg2017} for hyperparameter optimization. Explore multi-agent training with simultaneous learning of multiple agents.
% ===== END SECTION TO REWRITE =====


% ============================================================================
% APPENDIX
% ============================================================================
\appendix
\section{Detailed Benchmark Results}\label{app:benchmark-details}

Table~\ref{tab:detailed-benchmark} provides comprehensive evaluation metrics for TD3 and SAC algorithms across multiple evaluation scenarios. All evaluations were performed over 100 games with deterministic policies (no exploration noise).

\begin{table}[h]
\centering
\begin{tabular}{lccccc}
\toprule
\textbf{Algorithm} & \textbf{Training} & \textbf{vs Weak Win\%} & \textbf{vs Strong Win\%} & \textbf{vs Weak Tie\%} & \textbf{Avg Reward} \\
\midrule
\multirow{2}{*}{TD3} & Baseline (27.5k) & 88\% & 100\% & 1\% & 7.15 (weak) \\
 & Self-Play (97.5k) & 92\% & 100\% & 1\% & 7.03 (weak) \\
\midrule
SAC & Final (27.5k+) & 90-92\% & 98-100\% & ~1-2\% & ~7.0-7.2 (weak) \\
\bottomrule
\end{tabular}
\caption{Detailed benchmark results: TD3 shows consistent 100\% performance against strong opponents at both baseline and self-play stages. Self-play improves weak opponent performance from 88\% to 92\%. SAC achieves comparable performance with slightly lower strong opponent win rate (98-100\%). All evaluations use deterministic policies (no exploration noise) over 100 games.}
\label{tab:detailed-benchmark}
\end{table}

\section{Reward Shaping Components}\label{app:reward-shaping}

Table~\ref{tab:reward-components} details the reward shaping components used in our TD3 implementation. These components are based on domain knowledge of hockey strategy: offensive positioning near opponent goal, defensive positioning between puck and own goal, puck control, and risk management. We use potential-based reward shaping \cite{ng1999} to ensure policy optimality is preserved under reward transformations.

\begin{table}[h]
\centering
\begin{tabular}{llc}
\toprule
\textbf{Component} & \textbf{Description} & \textbf{Value/Range} \\
\midrule
\multicolumn{3}{l}{\textbf{PBRS Components ($\alpha = 0.005$)}} \\
\midrule
$\Phi_{\text{off}}$ & Offensive progress & $[0, 1.5]$ \\
$\Phi_{\text{prox}}$ & Puck proximity & $[-1.5, 0]$ \\
$\Phi_{\text{defense}}$ & Defensive lane positioning & $[0, 1.0]$ \\
$\Phi_{\text{cushion}}$ & Position cushion (risk penalty) & $[-2.0, 0]$ \\
\midrule
\multicolumn{3}{l}{\textbf{Strategic Bonuses}} \\
\midrule
Puck touch & Contact with puck & +0.06 \\
Proximity bonus & Fine-grained distance signal & +0.01 \\
Direction toward goal & Forward movement encouragement & +0.12 \\
Goal proximity & Final stage signal & +0.015 \\
Clear shot (unblocked) & Valuable strategic action & +0.15 \\
Blocked shot & Penalty for blocked attempts & -0.20 \\
Attack diversity & Per side (max +1.5) & +0.5 per side \\
Opponent forcing & Defensive effectiveness & +0.1 $\times$ dist\_moved \\
\bottomrule
\end{tabular}
\caption{Reward shaping components: PBRS potential functions and strategic bonuses. PBRS scaling factor $\alpha=0.005$ is conservative to prevent Q-value explosion while providing learning guidance. Total potential function $\Phi(s) = \Phi_{\text{off}} + \Phi_{\text{prox}} + \Phi_{\text{defense}} + \Phi_{\text{cushion}}$ is scaled by $\alpha$ and added to sparse reward: $r_{\text{shaped}} = r_{\text{sparse}} + \alpha \cdot \Phi(s)$. Strategic bonuses are added directly to reward signal.}
\label{tab:reward-components}
\end{table}

% ============================================================================
% REFERENCES
% ============================================================================
\newpage
\bibliographystyle{abbrv}
\bibliography{main}

\end{document}
