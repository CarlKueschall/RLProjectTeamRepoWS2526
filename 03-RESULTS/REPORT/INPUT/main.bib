% DreamerV3 and World Models
@article{hafner2024,
  title={Mastering Diverse Domains through World Models},
  author={Hafner, Danijar and Pasukonis, Jurgis and Ba, Jimmy and Lillicrap, Timothy},
  journal={arXiv preprint arXiv:2301.04104},
  year={2024}
}

@article{hafner2020dream,
  title={Dream to Control: Learning Behaviors by Latent Imagination},
  author={Hafner, Danijar and Lillicrap, Timothy and Ba, Jimmy and Norouzi, Mohammad},
  journal={arXiv preprint arXiv:1912.01603},
  year={2020}
}

@article{hafner2021dreamerv2,
  title={Mastering Atari with Discrete World Models},
  author={Hafner, Danijar and Lillicrap, Timothy and Norouzi, Mohammad and Ba, Jimmy},
  journal={arXiv preprint arXiv:2010.02193},
  year={2021}
}

% DreamSmooth
@article{dreamsmooth2023,
  title={DreamSmooth: Improving Model-based Reinforcement Learning via Reward Smoothing},
  author={Wu, Vint and Aslanides, John and Ba, Jimmy and Hafner, Danijar},
  journal={arXiv preprint arXiv:2311.01450},
  year={2023}
}

% Hockey Environment
@misc{martius2024,
  title={Hockey Environment},
  author={Martius, Georg and Kolev, Pavel and Zhobro, Mikel and Ramen, Sai},
  howpublished={\url{https://github.com/martius-lab/hockey-env}},
  year={2024}
}

% Gymnasium
@misc{towers2024,
  title={Gymnasium: A Standard Interface for Reinforcement Learning Environments},
  author={Towers, Mark and Kwiatkowski, Ariel and Terry, Jordan and Balis, John U and De Cola, Gianluca and Deleu, Tristan and Goul√£o, Manuel and Kallinteris, Andreas and Krimmel, Markus and Kang, Arjun and others},
  howpublished={\url{https://gymnasium.farama.org/}},
  year={2024}
}

% Self-Play and PFSP
@article{vinyals2019,
  title={Grandmaster level in StarCraft II using multi-agent reinforcement learning},
  author={Vinyals, Oriol and Babuschkin, Igor and Czarnecki, Wojciech M and Mathieu, Micha{\"e}l and Dudzik, Andrew and Chung, Junyoung and Choi, David H and Powell, Richard and Ewalds, Timo and Georgiev, Petko and others},
  journal={Nature},
  volume={575},
  number={7782},
  pages={350--354},
  year={2019},
  publisher={Nature Publishing Group}
}

% TD3
@inproceedings{fujimoto2018,
  title={Addressing Function Approximation Error in Actor-Critic Methods},
  author={Fujimoto, Scott and van Hoof, Herke and Meger, David},
  booktitle={International Conference on Machine Learning},
  pages={1587--1596},
  year={2018},
  organization={PMLR}
}

% SAC
@inproceedings{haarnoja2018,
  title={Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor},
  author={Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
  booktitle={International Conference on Machine Learning},
  pages={1861--1870},
  year={2018},
  organization={PMLR}
}

% TD-MPC
@inproceedings{hansen2024,
  title={TD-MPC2: Scalable, Robust World Models for Continuous Control},
  author={Hansen, Nicklas and Su, Hao and Wang, Xiaolong},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2024}
}

% Rainbow DQN
@inproceedings{hessel2018,
  title={Rainbow: Combining Improvements in Deep Reinforcement Learning},
  author={Hessel, Matteo and Modayil, Joseph and Van Hasselt, Hado and Schaul, Tom and Ostrovski, Georg and Dabney, Will and Horgan, Dan and Piot, Bilal and Azar, Mohammad and Silver, David},
  booktitle={AAAI Conference on Artificial Intelligence},
  year={2018}
}

% PBRS
@inproceedings{ng1999,
  title={Policy Invariance Under Reward Transformations: Theory and Application to Reward Shaping},
  author={Ng, Andrew Y and Harada, Daishi and Russell, Stuart},
  booktitle={International Conference on Machine Learning},
  pages={278--287},
  year={1999}
}

% PyTorch
@inproceedings{paszke2019,
  title={PyTorch: An Imperative Style, High-Performance Deep Learning Library},
  author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  booktitle={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

% W&B
@misc{biewald2020,
  title={Experiment Tracking with Weights and Biases},
  author={Biewald, Lukas},
  howpublished={\url{https://www.wandb.com/}},
  year={2020}
}

% CrossQ
@inproceedings{bhatt2024,
  title={CrossQ: Batch Normalization in Deep Reinforcement Learning for Greater Sample Efficiency and Simplicity},
  author={Bhatt, Aditya and Palenicek, Daniel and Belousov, Boris and Argus, Max and Amiranashvili, Artemij and Brox, Thomas and Peters, Jan},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2024}
}

% Pink Noise
@inproceedings{eberhard2023,
  title={Pink Noise Is All You Need: Colored Noise Exploration in Deep Reinforcement Learning},
  author={Eberhard, Onno and Hollenstein, Jakob and Pinneri, Cristina and Martius, Georg},
  booktitle={Proceedings of the Eleventh International Conference on Learning Representations},
  year={2023}
}

% RND
@inproceedings{burda2019,
  title={Exploration by Random Network Distillation},
  author={Burda, Yuri and Edwards, Harrison and Storkey, Amos and Klimov, Oleg},
  booktitle={International Conference on Learning Representations},
  year={2019}
}

% DDPG
@article{lillicrap2015,
  title={Continuous Control with Deep Reinforcement Learning},
  author={Lillicrap, Timothy P and Hunt, Jonathan J and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
  journal={arXiv preprint arXiv:1509.02971},
  year={2015}
}

% Dueling DQN
@inproceedings{wang2016,
  title={Dueling Network Architectures for Deep Reinforcement Learning},
  author={Wang, Ziyu and Schaul, Tom and Hessel, Matteo and Hasselt, Hado and Lanctot, Marc and Freitas, Nando},
  booktitle={International Conference on Machine Learning},
  pages={1995--2003},
  year={2016},
  organization={PMLR}
}
